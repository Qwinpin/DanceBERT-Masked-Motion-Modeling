{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/rotation_matrix.300.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array([i for i in data if i['type'] == 'train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poses and music normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global coordinates are not scaled to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalization = {}\n",
    "\n",
    "for i in range(216, 219):\n",
    "    tmp = np.concatenate([j['pose'] for j in data], 0)[:, i]\n",
    "    tmp_mean = tmp.mean()\n",
    "    tmp_std = tmp.std()\n",
    "    tmp_min = tmp.min()\n",
    "    tmp_max = tmp.max()\n",
    "\n",
    "    if data_normalization.get('pose', None) is None:\n",
    "        data_normalization['pose'] = {}\n",
    "    if data_normalization['pose'].get(i, None) is None:\n",
    "        data_normalization['pose'][i] = {}\n",
    "    data_normalization['pose'][i]['mean'] = tmp_mean\n",
    "    data_normalization['pose'][i]['std'] = tmp_std\n",
    "    data_normalization['pose'][i]['min'] = tmp_min\n",
    "    data_normalization['pose'][i]['max'] = tmp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if data_normalization.get('music', None) is None:\n",
    "    data_normalization['music'] = {}\n",
    "\n",
    "for i in range(35):\n",
    "    tmp = np.concatenate([i['music'] for i in data], 0)[:, i]\n",
    "    data_normalization['music'][i] = {}\n",
    "    data_normalization['music'][i]['mean'] = tmp.mean()\n",
    "    data_normalization['music'][i]['std'] = tmp.std()\n",
    "    data_normalization['music'][i]['min'] = tmp.min()\n",
    "    data_normalization['music'][i]['max'] = tmp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_train_poses(sample):\n",
    "    for i in range(216, 219):\n",
    "        sample[:, :, i] = (sample[:, :, i] - data_normalization['pose'][i]['min']) / (data_normalization['pose'][i]['max'] - data_normalization['pose'][i]['min'])\n",
    "\n",
    "    return sample\n",
    "\n",
    "def norm_train_music(sample):\n",
    "    for i in range(35):\n",
    "        sample[:, :, i] = (sample[:, :, i] - data_normalization['music'][i]['min']) / (data_normalization['music'][i]['max'] - data_normalization['music'][i]['min'])\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unnorm_train_poses(sample):\n",
    "    for i in range(216, 219):\n",
    "            sample[:, :, i] = sample[:, :, i] * (data_normalization['pose'][i]['max'] - data_normalization['pose'][i]['min']) + data_normalization['pose'][i]['min']\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert for poses processing\n",
    "\n",
    "bert_config = transformers.ConvBertConfig().to_dict()\n",
    "config = transformers.PretrainedConfig().to_dict()\n",
    "\n",
    "bert_config['num_hidden_layers'] = 5\n",
    "bert_config['num_attention_heads'] = 10\n",
    "bert_config['output_hidden_states'] = True\n",
    "bert_config['hidden_size'] = 800\n",
    "bert_config['embedding_size'] = 800\n",
    "bert_config['max_position_embeddings'] = 300\n",
    "# bert_config['type_vocab_size'] = 24\n",
    "\n",
    "bert_config = transformers.ConvBertConfig().from_dict(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert for music processing\n",
    "\n",
    "bert_config2 = transformers.ConvBertConfig().to_dict()\n",
    "config = transformers.PretrainedConfig().to_dict()\n",
    "\n",
    "bert_config2['num_hidden_layers'] = 5\n",
    "bert_config2['num_attention_heads'] = 10\n",
    "bert_config2['output_hidden_states'] = True\n",
    "bert_config2['hidden_size'] = 800\n",
    "bert_config2['embedding_size'] = 800\n",
    "bert_config2['max_position_embeddings'] = 300\n",
    "# bert_config['type_vocab_size'] = 24\n",
    "\n",
    "bert_config2 = transformers.ConvBertConfig().from_dict(bert_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-modal bert\n",
    "\n",
    "bert_config3 = transformers.ConvBertConfig().to_dict()\n",
    "config = transformers.PretrainedConfig().to_dict()\n",
    "\n",
    "bert_config3['num_hidden_layers'] = 5\n",
    "bert_config3['num_attention_heads'] = 10\n",
    "bert_config3['output_hidden_states'] = True\n",
    "bert_config3['hidden_size'] = 1600\n",
    "bert_config3['embedding_size'] = 1600\n",
    "bert_config3['max_position_embeddings'] = 300\n",
    "# bert_config['type_vocab_size'] = 24\n",
    "\n",
    "bert_config3 = transformers.ConvBertConfig().from_dict(bert_config3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseBERT(torch.nn.Module):\n",
    "    def __init__(self, bert_config, bert_config2, bert_config3):\n",
    "        super(PoseBERT, self).__init__()\n",
    "        self.bert = transformers.ConvBertModel(bert_config)\n",
    "        self.bert2 = transformers.ConvBertModel(bert_config2)\n",
    "        self.bert3 = transformers.ConvBertModel(bert_config3)\n",
    "        \n",
    "        self.linear = torch.nn.Linear(800 * 2, 219)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def single_pass(self, xp, xm, attention_mask):\n",
    "        # simply pad input data to bert hidden size\n",
    "        xp = torch.nn.functional.pad(xp, (0, 800 - xp.shape[-1]))\n",
    "        xm = torch.nn.functional.pad(xm, (0, 800 - xm.shape[-1]))\n",
    "        \n",
    "        poses_output_seed = self.bert(inputs_embeds=xp, attention_mask=attention_mask)['last_hidden_state']\n",
    "        music_output_seed = self.bert2(inputs_embeds=xm, attention_mask=attention_mask)['last_hidden_state']\n",
    "        \n",
    "        # concatenate processed music and poses together\n",
    "        predict = torch.cat([poses_output_seed, music_output_seed], -1)\n",
    "\n",
    "        _predict = self.bert3(inputs_embeds=predict, attention_mask=attention_mask)['last_hidden_state']\n",
    "        \n",
    "        decoder_output_pose = self.activation(self.linear(_predict))\n",
    "\n",
    "        return decoder_output_pose\n",
    "    \n",
    "    def forward(self, input_xp, input_xm, attention_mask, device='cpu'):\n",
    "            predict = self.single_pass(input_xp, input_xm, attention_mask)\n",
    "\n",
    "            return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poseBert = PoseBERT(bert_config, bert_config2, bert_config3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history arrays\n",
    "G_losses = []\n",
    "\n",
    "iters = 0\n",
    "epochs = 950\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "lrG = 1e-4\n",
    "beta1 = 0.9\n",
    "\n",
    "optimizerG = torch.optim.Adam(poseBert.parameters(), lr=lrG, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_seed = 300\n",
    "frames_music = 300\n",
    "frames_min_length = 120\n",
    "\n",
    "# probabilities according to BERT paper\n",
    "masked_probability = 0.15\n",
    "zero_probability = 0.80\n",
    "replace_probability = 0.10\n",
    "same_probability = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_masked_batch(pose_batch):\n",
    "    # clone input tensors\n",
    "    masked_pose_batch = torch.tensor(pose_batch)\n",
    "    original_pose_batch = torch.tensor(pose_batch)\n",
    "    \n",
    "    # assume all sequences should be different\n",
    "    samples_lengths = np.random.randint(frames_min_length, frames_seed, (pose_batch.shape[0]))\n",
    "    masks = np.ones((pose_batch.shape[0], frames_seed))\n",
    "    attention_mask = np.ones((pose_batch.shape[0], frames_seed))\n",
    "\n",
    "    # cut sequences and apply masking\n",
    "    for j, sample_length in enumerate(samples_lengths):\n",
    "        attention_mask[j][sample_length:] *= 0\n",
    "        \n",
    "        # mask or not\n",
    "        mask = np.random.choice([1, 0], p=[1-masked_probability, masked_probability], size=(sample_length))\n",
    "        masks[j][:len(mask)] *= mask\n",
    "        \n",
    "        # replace with zero, random other element or dont change\n",
    "        mask_type = np.random.choice([0, 1, 2], p=[zero_probability, replace_probability, same_probability], size=(mask == 0).sum())\n",
    "        \n",
    "        masked_pose_batch[j][sample_length:] *= 0\n",
    "        for i, index in enumerate(np.where(mask == 0)[0]):\n",
    "            if mask_type[i] == 0:\n",
    "                masked_pose_batch[j][index] *= 0\n",
    "\n",
    "            elif mask_type[i] == 1:\n",
    "                select_sample_to_replace = np.random.randint(pose_batch.shape[0])\n",
    "                lst = np.arange(samples_lengths[select_sample_to_replace])\n",
    "                lst = lst[np.where(lst != index)]\n",
    "                random_replace = np.random.choice(lst)\n",
    "\n",
    "                masked_pose_batch[j][index] = original_pose_batch[select_sample_to_replace][random_replace]\n",
    "\n",
    "    return masked_pose_batch, masks[:, :, None].repeat(pose_batch.shape[-1], -1), attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used for evaluation with fixed setting\n",
    "\n",
    "def generate_masked_batch_patched(pose_batch, pattern=0):\n",
    "    masked_pose_batch = torch.tensor(pose_batch)\n",
    "    original_pose_batch = torch.tensor(pose_batch)\n",
    "\n",
    "    samples_lengths = np.array([100 for _ in range(pose_batch.shape[0])])\n",
    "    masks = np.ones((pose_batch.shape[0], frames_seed))\n",
    "    attention_mask = np.ones((pose_batch.shape[0], frames_seed))\n",
    "\n",
    "    for j, sample_length in enumerate(samples_lengths):\n",
    "        attention_mask[j][sample_length:] *= 0\n",
    "        \n",
    "        # recover first pose of the sequence\n",
    "        if pattern == 0:\n",
    "            mask = np.ones((sample_length))\n",
    "            mask[0] = 0\n",
    "        # 5 over 5\n",
    "        elif pattern == 1:\n",
    "            tmp = [1] * 5 + [0] * 5\n",
    "            mask = np.array((tmp * 30)[:sample_length])\n",
    "        # recover using only 1 per 5\n",
    "        elif pattern == 2:\n",
    "            mask = np.zeros((sample_length))\n",
    "            mask[::5] = 1\n",
    "        # recover set of blank poses\n",
    "        elif pattern == 3:\n",
    "            mask = np.ones((sample_length))\n",
    "            mask[88:98] = 0\n",
    "\n",
    "        masks[j][:len(mask)] *= mask\n",
    "        mask_type = np.random.choice([0, 1, 2], p=[zero_probability, replace_probability, same_probability], size=(mask == 0).sum())\n",
    "        \n",
    "        masked_pose_batch[j][sample_length:] *= 0\n",
    "        for i, index in enumerate(np.where(mask == 0)[0]):\n",
    "            if mask_type[i] == 0:\n",
    "                masked_pose_batch[j][index] *= 0\n",
    "\n",
    "            elif mask_type[i] == 1:\n",
    "                select_sample_to_replace = np.random.randint(pose_batch.shape[0])\n",
    "                lst = np.arange(samples_lengths[select_sample_to_replace])\n",
    "                lst = lst[np.where(lst != index)]\n",
    "                random_replace = np.random.choice(lst)\n",
    "\n",
    "                masked_pose_batch[j][index] = original_pose_batch[select_sample_to_replace][random_replace]\n",
    "                \n",
    "    return masked_pose_batch, masks[:, :, None].repeat(pose_batch.shape[-1], -1), attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # shuffle training data\n",
    "    indexes = np.array(list(range(len(data_train))))\n",
    "    np.random.shuffle(indexes)\n",
    "    _data_train = data_train[indexes]\n",
    "\n",
    "    data_batches = np.array_split(_data_train[:len(data_train) // batch_size * batch_size], \n",
    "                                  len(_data_train[:len(data_train) // batch_size * batch_size])//batch_size)\n",
    "\n",
    "    for i, batch in enumerate(data_batches):\n",
    "        pose_batch = norm_train_poses(torch.tensor(np.concatenate([i['pose'][None] for i in batch], 0))).to(device).float()\n",
    "        music_batch = norm_train_music(torch.tensor(np.concatenate([i['music'][None] for i in batch], 0))).to(device).float()\n",
    "        \n",
    "        masked_pose_batch, mask, attention_mask = generate_masked_batch(pose_batch.clone().detach())\n",
    "        \n",
    "        fake = poseBert(masked_pose_batch, music_batch, torch.tensor(attention_mask).to(device), device=device, train=True)\n",
    "        \n",
    "        # apply mask to real and generated sequences in order to assess loss only for masked frames\n",
    "        errG = criterion(fake.masked_fill(torch.tensor(mask).to(device).bool(), 0), pose_batch.masked_fill(torch.tensor(mask).to(device).bool(), 0))\n",
    "        optimizerG.zero_grad()\n",
    "        errG.backward()\n",
    "        iters += 1\n",
    "        G_losses.append(errG.item())\n",
    "\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if iters % 10 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_G: %.5f'\n",
    "                  % (epoch, epochs, i, len(data_batches),\n",
    "                     errG.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pose_batch = norm_train_poses(torch.tensor(np.concatenate([i['pose'][None] for i in [i for i in data if i['type'] == 'test'][:10]], 0))).to(device).float()\n",
    "music_batch = norm_train_music(torch.tensor(np.concatenate([i['music'][None] for i in [i for i in data if i['type'] == 'test'][:10]], 0))).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_masked_pose_batch, _mask, _attention_mask = generate_masked_batch_patched(pose_batch, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence 7 is a good demostration of complex motion\n",
    "with torch.no_grad():\n",
    "    fake = poseBert(_masked_pose_batch.clone().detach()[7:8], music_batch.clone().detach()[7:8], torch.tensor(_attention_mask).clone().detach()[7:8].to(device), device=device, train=True)\n",
    "\n",
    "# [:, :100] - select only first 100 frames\n",
    "# [7] - select sequence 7\n",
    "# [:, :-3] - remove global coordinates since our current visualization method does not support them\n",
    "_fake = torch.cat([unnorm_train_poses(pose_batch[:, :100])[7][:, :-3], unnorm_train_poses(fake)[:, :100][0][:, :-3]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
